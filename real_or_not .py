# -*- coding: utf-8 -*-
"""real_or_not.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M71LNDUUk8v5mwclmyGhho1_ew19OwKH
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import random
import tensorflow as tf
from tensorflow.keras.preprocessing.text import  Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
# import nltk
# nltk.download('all')

# from sklearn.model_selection import train_test_split
# from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer

df_train = pd.read_csv('/content/drive/My Drive/nlp-getting-started/train.csv')
df_test = pd.read_csv('/content/drive/My Drive/nlp-getting-started/test.csv')

print('train length : {}'.format(len(df_train)))
print('test length : {}'.format(len(df_test)))

oov_tok = '<OOV>'
training_size = 6000
padding = 'post'
trunk = 'post'
embed_size = 50
max_length = 64
vocab_size = 10_000

# df_train = df_train[['text','target']]

x = df_train['text']
y = df_train['target']
x_t = df_test['text']
# x_train,x_test,y_train,y_test = train_test_split(x,y)
random.shuffle(x)
tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)
tokenizer.fit_on_texts(x)
word_index = tokenizer.word_index
# vocab_size = len(word_index)

sequences = tokenizer.texts_to_sequences(x) 
padded = pad_sequences(sequences,padding=padding,truncating=trunk,maxlen=max_length)

train_sent = padded[:training_size]
val_sent = padded[training_size:]
train_labels = y[:training_size]
val_labels = y[training_size:]
train_sent

# from sklearn.linear_model import LogisticRegression
# from sklearn.naive_bayes import MultinomialNB
# from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier
# from sklearn.svm import  SVC
# vect = TfidfVectorizer(min_df=5,ngram_range=(1,3)).fit(x_train)
# x_train_vect = vect.transform(x_train)

# clf = LogisticRegression().fit(x_train_vect,y_train)
# # clf = MultinomialNB(alpha=0.1).fit(x_train_vect,y_train)
# # clf = ExtraTreesClassifier().fit(x_train_vect,y_train)
# # clf = SVC(C=1000).fit(x_train_vect,y_train)


# print(clf.score(vect.transform(x_test),y_test))
# # print(len(y_test))

# path = ''

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size,embed_size, input_length=max_length),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Conv1D(64, 5, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(48, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(12, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')


])
model.summary()
model.compile(optimizer='adam',loss='mse',metrics=['acc'])

# y_t = clf.predict(vect.transform(x_t))
num_epochs=10
hist = model.fit(train_sent,train_labels,validation_data=[val_sent,val_labels],epochs=num_epochs,verbose=1)

import matplotlib.image  as mpimg
import matplotlib.pyplot as plt
history = hist
acc=history.history['acc']
val_acc=history.history['val_acc']
loss=history.history['loss']
val_loss=history.history['val_loss']

epochs=range(len(acc))

plt.plot(epochs, acc, 'r')
plt.plot(epochs, val_acc, 'b')
plt.title('Training and validation accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["Accuracy", "Validation Accuracy"])

plt.figure()


plt.plot(epochs, loss, 'r')
plt.plot(epochs, val_loss, 'b')
plt.title('Training and validation loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(["Loss", "Validation Loss"])

plt.figure()

seq_t = tokenizer.texts_to_sequences(x_t)
pad_t = pad_sequences(seq_t,padding =padding,truncating=trunk,maxlen=max_length)

y_t = model.predict(pad_t)

for i in range(len(y_t)):
  if y_t[i]>0.50:
    y_t[i]=1
  else:
    y_t[i]=0
y_t

idx = df_test['id'].values
new_pred = np.asarray(y_t,dtype=np.int64).reshape(-1)
print(idx.shape)

new_pred

data = {'id':idx , 'target':new_pred}
ret = pd.DataFrame(data)

ret = ret.set_index('id')

ret.to_csv('submit.csv')

ret

# clf.predict(vect.transform(['Breaking News, india attacks pakistan']))

# a = np.max(df_test['text'])
# len(a)

